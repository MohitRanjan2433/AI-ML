{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNWI9k6foAsNniew1ffTOOM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"4mXsYqm_1PhH","executionInfo":{"status":"ok","timestamp":1703341005729,"user_tz":-330,"elapsed":464,"user":{"displayName":"Mohit Ranjan","userId":"08369355047002042713"}}},"outputs":[],"source":["from nltk.corpus import stopwords\n","from nltk.cluster.util import cosine_distance\n","import numpy as np\n","import networkx as nx"]},{"cell_type":"markdown","source":["Generate clean sentences"],"metadata":{"id":"lmuqzgdD68Uf"}},{"cell_type":"code","source":["def read_article(file_name):\n","  file = open(file_name, \"r\")\n","  filedata = file.readlines()\n","  article = filedata[0].split(\". \")\n","  sentences = []\n","\n","  for sentence in article:\n","    print(sentence)\n","    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n","    sentences.pop()\n","\n","  return sentences"],"metadata":{"id":"7_mIXaKb28bj","executionInfo":{"status":"ok","timestamp":1703341008514,"user_tz":-330,"elapsed":5,"user":{"displayName":"Mohit Ranjan","userId":"08369355047002042713"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Similarity between the sentences"],"metadata":{"id":"kW0lczKhEnDs"}},{"cell_type":"code","source":["def sentence_similarity(sent1, sent2, stopwords=None):\n","  if stopwords is None:\n","    stopwords = []\n","\n","\n","  sent1 = [w.lower() for w in sent1]\n","  sent2 = [w.lower() for w in sent2]\n","\n","  all_words = list(set(sent1 + sent2))\n","\n","  vector1 = [0] * len(all_words)\n","  vector2 = [0] * len(all_words)\n","\n","  #build the vector for the first sentence\n","  for w in sent1:\n","    if w in stopwords:\n","      continue\n","    vector1[all_words.index(w)] += 1\n","\n","  #build the vector for the second sentence\n","  for w in sent2:\n","    if w in stopwords:\n","      continue\n","    vector2[all_words.index(w)] += 1"],"metadata":{"id":"luxHtWuu74Uj","executionInfo":{"status":"ok","timestamp":1703341032610,"user_tz":-330,"elapsed":439,"user":{"displayName":"Mohit Ranjan","userId":"08369355047002042713"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Generating the similarity matrix"],"metadata":{"id":"QiXAbEkaErbD"}},{"cell_type":"code","source":["def build_similarity_matrix(sentences, stop_words):\n","  #create an empty similarity matrix\n","  similarity_matrix = np.zeros((len(sentences), len(sentences)))\n","\n","  for idx1 in range(len(sentences)):\n","    for idx2 in range(len(sentences)):\n","      if idx1 == idx2:\n","        continue\n","      similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2])\n","\n","  return similarity_matrix"],"metadata":{"id":"KYGUx21jCYzP","executionInfo":{"status":"ok","timestamp":1703341038335,"user_tz":-330,"elapsed":2,"user":{"displayName":"Mohit Ranjan","userId":"08369355047002042713"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Generate summary method"],"metadata":{"id":"kvXVFbFEEzxT"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.cluster.util import cosine_distance\n","import numpy as np\n","import networkx as nx\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","\n","def read_article(file_name):\n","    file = open(file_name, \"r\")\n","    filedata = file.read()\n","    sentences = nltk.sent_tokenize(filedata)\n","    sentences = [sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \") for sentence in sentences]\n","    return sentences\n","\n","def sentence_similarity(sent1, sent2, stopwords=None):\n","    if stopwords is None:\n","        stopwords = []\n","\n","    sent1 = [w.lower() for w in sent1]\n","    sent2 = [w.lower() for w in sent2]\n","\n","    all_words = list(set(sent1 + sent2))\n","\n","    vector1 = [0] * len(all_words)\n","    vector2 = [0] * len(all_words)\n","\n","    # build the vector for the first sentence\n","    for w in sent1:\n","        if w in stopwords:\n","            continue\n","        vector1[all_words.index(w)] += 1\n","\n","    # build the vector for the second sentence\n","    for w in sent2:\n","        if w in stopwords:\n","            continue\n","        vector2[all_words.index(w)] += 1\n","\n","    return 1 - cosine_distance(vector1, vector2)\n","\n","def build_similarity_matrix(sentences, stop_words):\n","    # Create an empty similarity matrix\n","    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n","\n","    for idx1 in range(len(sentences)):\n","        for idx2 in range(len(sentences)):\n","            if idx1 == idx2: #ignore if both are same sentences\n","                continue\n","            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n","\n","    return similarity_matrix\n","\n","\n","def generate_summary(file_name, output_file, top_n=2):\n","    stop_words = stopwords.words('english')\n","    summarize_text = []\n","\n","    # Step 1 - Read text anc split it\n","    sentences =  read_article(file_name)\n","\n","    # Step 2 - Generate Similary Martix across sentences\n","    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n","\n","    # Step 3 - Rank sentences in similarity martix\n","    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n","    scores = nx.pagerank(sentence_similarity_graph)\n","\n","    # Step 4 - Sort the rank and pick top sentences\n","    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n","    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n","\n","    for i in range(top_n):\n","      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n","\n","      # Step 5 - Output the summarized text to a new file\n","    with open(output_file, \"w\") as summary_file:\n","        summary_file.write(\". \".join(summarize_text))\n","\n","    print(f\"Summary has been saved in '{output_file}'\")\n","\n","# let's begin\n","generate_summary( \"new.txt\",\"new_summary1.txt\", 2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8An8WwLJCIq4","executionInfo":{"status":"ok","timestamp":1703342941573,"user_tz":-330,"elapsed":9,"user":{"displayName":"Mohit Ranjan","userId":"08369355047002042713"}},"outputId":"39ef16bd-3804-4fbe-d084-dbb5f42e9438"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Indexes of top ranked_sentence order are  [(0.0759334791499862, ['The', 'paper', 'narrows', 'the', 'scope', 'to', 'the', 'issue', 'of', 'drug', 'abuse', 'and', 'addiction', 'since', 'this', 'is', 'arguably', 'the', 'basic', 'source', 'of', 'information', 'for', 'the', 'other', 'forms', 'of', 'addictions', 'as', 'well.']), (0.06169697254346787, ['Thesis', 'Statement\\n“The', 'affects', 'of', 'drug', 'addictions', 'and', 'its', 'influence', 'on', 'other', 'forms', 'of', 'everyday', 'addictions”', 'This', 'paper', 'is', 'an', 'analysis', 'of', 'the', 'effect', 'of', 'everyday', 'addictions', 'its', 'effects', 'and', 'possibilities', 'as', 'well', 'as', 'implications', 'over', 'future', 'endeavours.']), (0.05405318381129114, ['Arguably', 'the', 'other', 'forms', 'of', 'addictions', 'are', 'often', 'side', 'effects', 'of', 'drug', 'abuse', 'and', 'addiction.']), (0.0483482436746138, ['The', 'paper', 'also', 'forms', 'an', 'analysis', 'over', 'issue', 'of', 'other', 'probable', 'cause', 'in', 'the', 'subject', 'matter.']), (0.047693846787219545, ['This', 'implies', 'that', 'addiction', 'is', 'an', 'everyday', 'life', 'situation', 'and', 'therefore', 'addicts', 'are', 'common', 'people', 'suffering', 'due', 'to', 'lack', 'of', 'support', 'from', 'friends.']), (0.043347340049440006, ['Objective/significance', 'of', 'the', 'study\\nThe', 'main', 'objective', 'of', 'the', 'paper', 'focuses', 'on', 'forms', 'of', 'addictions', 'and', 'ways', 'of', 'evaluating', 'the', 'developmental', 'problems', 'associated', 'with', 'this', 'type', 'of', 'vice.']), (0.04263412598004014, ['Food', 'might', 'be', 'one', 'of', 'the', 'hardest', 'addictions', 'to', 'deal', 'with', 'since', 'it', 'is', 'a', 'basic', 'need', 'for', 'the', 'survival', 'of', 'living', 'things.']), (0.03878318150804597, ['Lastly,', 'it', 'addresses', 'the', 'issue', 'of', 'utilizing', 'the', 'new', 'professionally', 'suggested', 'measures', 'to', 'breaking', 'from', 'this', 'common', 'phenomenon', 'of', 'addictions.']), (0.03579695644322249, ['The', 'analysis', 'of', 'what', 'determines', 'addictions.']), (0.03527038333526712, ['Addiction', 'to', 'food', 'requires', 'urgent', 'assistance', 'to', 'avoid', 'eating', 'disorders', 'just', 'like', 'the', 'alcoholics', 'it', 'would', 'require', 'some', 'form', 'of', 'therapy', 'at', 'rehabilitation', 'centres.']), (0.03513564893242167, ['The', 'minimum', 'time', 'our', 'certified', 'writers', 'need', 'to', 'deliver\\na', '100%', 'original', 'paper\\nThe', 'common', 'addictions', 'today', 'are', 'hard', 'to', 'control', 'because', 'they', 'involve', 'inevitable', 'choice', 'of', 'living', 'such', 'as', 'reading,', 'playing', 'and', 'eating.']), (0.034479055597342906, ['One', 'is', 'not', 'in', 'a', 'position', 'to', 'stop', 'eating', 'abruptly', 'as', 'compared', 'to', 'other', 'forms', 'of', 'addictions.']), (0.03301018288668687, ['Although', 'addiction', 'is', 'a', 'negative', 'influence', 'over', 'personality,', 'depending', 'on', 'the', 'type', 'it', 'can', 'be', 'a', 'positive', 'force', 'supporting', 'the', 'decision', 'making', 'procedures', 'and', 'helping', 'to', 'settle', 'for', 'the', 'best', 'option', 'in', 'tough', 'situations', 'for', 'instance', 'addiction', 'to', 'reading.']), (0.03250665208971856, ['Drug', 'addiction', 'carries', 'enormous', 'quantity', 'of', 'ambiguity', 'and', 'brings', 'about', 'misunderstandings', 'due', 'to', 'huge', 'amount', 'of', 'information', 'required', 'to', 'address', 'its', 'related', 'problems.']), (0.031058426830745518, ['The', 'technological', 'advancement', 'today', 'requires', 'everyone', 'to', 'be', 'in', 'apposition', 'of', 'using', 'the', 'computers', 'and', 'the', 'network', 'and', 'addiction', 'to', 'its', 'use', 'can', 'equally', 'tear', 'off', 'the', 'abusers', 'lives', 'just', 'like', 'the', 'alcohol', 'or', 'tobacco', 'would', 'do.']), (0.02986912176302255, ['(Matthynssens,', '2009)', 'It', 'is', 'common', 'to', 'find', 'a', 'person', 'who', 'is', 'addicted', 'to', 'pornography', 'due', 'to', 'other', 'substance', 'abuse', 'and', 'also', 'often', 'to', 'find', 'someone', 'trying', 'to', 'find', 'other', 'alternatives', 'in', 'the', 'aim', 'of', 'fighting', 'alcohol', 'or', 'tobacco', 'addiction.']), (0.02679498597788631, ['Addicts', 'need', 'to', 'ensure', 'they', 'are', 'around', 'caring', 'and', 'happy', 'people', 'for', 'the', 'support', 'they', 'need', 'for', 'a', 'successful', 'quit.']), (0.02677066509536706, ['Any', 'form', 'of', 'addiction', 'is', 'enslaving', 'because', 'it', 'prevents', 'achievement', 'of', 'goals', 'by', 'affecting', 'participation', 'in', 'healthy', 'and', 'more', 'productive', 'activities.']), (0.025554206095274454, ['There', 'are', 'various', 'types', 'of', 'addictions', 'ranging', 'from', 'food,', 'drugs,', 'sex,', 'play,', 'work', 'and', 'many', 'more.']), (0.024680295064680408, ['The', 'analysis', 'then', 'draws', 'the', 'conclusion', 'from', 'generally', 'analyzed', 'data', 'in', 'the', 'literature', 'review.']), (0.024560813955013323, ['The', 'first', 'cure', 'to', 'addiction', 'is', 'friends', 'and', 'ability', 'to', 'enjoy', 'a', 'moment', 'of', 'laughter.']), (0.023883678733888407, ['It', 'is', 'an', 'analysis', 'of', 'the', 'behavioural', 'patterns', 'and', 'lastly', 'it', 'analyzes', 'the', 'available', 'perspectives', 'into', 'breaking', 'the', 'vice.']), (0.02374961851633923, ['Being', 'addicted', 'to', 'the', 'internet', 'is', 'a', 'very', 'common', 'phenomenon', 'today', 'especially', 'among', 'the', 'youth.']), (0.022516189079385927, ['In', 'future,', 'it', 'is', 'probably', 'going', 'to', 'be', 'harder', 'than', 'food', 'because', 'eventually', 'we', 'all', 'need', 'to', 'use', 'the', 'net', 'for', 'communication', 'as', 'well', 'as', 'getting', 'access', 'to', 'important', 'information.']), (0.02251292609177238, ['Another', 'significance', 'of', 'the', 'study', 'focuses', 'on', 'the', 'global', 'approach', 'towards', 'curbing', 'the', 'vice', 'and', 'creating', 'public', 'awareness', 'over', 'everyday', 'addiction.']), (0.018210024517577268, ['It', 'also', 'involves', 'entertainment', 'such', 'as', 'watching', 'television,', 'browsing', 'the', 'net', 'and', 'many', 'more.']), (0.01748902321301241, ['Information', 'collected', 'tabulates', 'and', 'ranks', 'the', 'findings', 'to', 'broad', 'areas', 'and', 'helps', 'to', 'narrow', 'the', 'scope', 'to', 'the', 'objectives', 'of', 'the', 'study', 'analysis.']), (0.017037070231161845, ['Are', 'people', 'utilizing', 'the', 'appropriate', 'measures', 'to', 'get', 'rid', 'of', 'the', 'vice?']), (0.015546389437499565, ['When', 'one', 'accepts', 'to', 'put', 'up', 'with', 'negative', 'addiction,', 'they', 'end', 'up', 'giving', 'up', 'the', 'personal', 'trusts', 'and', 'values', 'thus', 'the', 'pressure', 'becomes', 'a', 'form', 'of', 'a', 'negative', 'force.']), (0.011392851282459343, ['The', 'Method', 'of', 'the', 'study\\nThe', 'literature', 'reviews', 'will', 'enable', 'better', 'understanding', 'of', 'the', 'topic.']), (0.01009979678940845, ['Preparation', 'of', 'the', 'research', 'proposal', 'over', 'the', 'chosen', 'topic', 'enhances', 'and', 'quantifies', 'the', 'research', 'as', 'a', 'study', 'topic', 'and', 'prepares', 'for', 'respondents.']), (0.004792332268370616, ['It', 'is', 'a', 'continuous', 'battle', 'to', 'keep', 'to', 'a', 'certain', 'limit.']), (0.004792332268370616, ['1', 'hour!'])]\n","Summary has been saved in 'new_summary1.txt'\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]}]}